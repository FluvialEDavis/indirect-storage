---
title: "Estimation of Direct and Indirect Storage, Ramuschaka"
author: "Edward Davis"
date: "2/19/2019"
output:
  html_document:
    css: styles.css
    highlight: pygments
    theme: sandstone
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
library(knitr)
```
### Libraries used
```{r libraries, message=FALSE}
library(tidyverse)
library(readxl)
library(lubridate)
library(padr)
library(tidyquant)
library(scales)
library(sirad)
library(pracma)
```

```{r graphing-stuff, include=FALSE}
extrafont::loadfonts(quiet = TRUE)
hydro_theme <- function() {
  theme_bw(base_size = 12, base_family = "IBM Plex Sans") %+replace%
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank(),
      plot.title = element_text(size = 15, face = "bold", hjust = 0, 
                                vjust = 2)
    )
}
theme_set(hydro_theme())

cb_palette <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```
### Data import and basic tidying
```{r data-import}
# Import data
ramu_precip <- read_excel("data/upper-middle.xlsx", sheet = "precip", skip = 1)
ramu_stage <- read_excel("data/upper-middle.xlsx", sheet = "stage", skip = 1)

# Data tidying
upper_middle <- ramu_precip %>%
  right_join(ramu_stage, by = "Date Time, GMT-05:00") %>%
  rename(date_time = `Date Time, GMT-05:00`,
         air_temp = `Temp, Â°C (LGR S/N: 20381067, SEN S/N: 20381067, LBL: T)`,
         water_level_m = `Water Level (m)`,
         q_est = `Q est m3/s`,
         precip_mm = `mm of rain`) %>%
  select(date_time, air_temp, precip_mm, water_level_m, q_est)

# Display first few rows of tidied data
kable(head(upper_middle))
```

# Recession Analysis
### Smoothing gage data
Here, I resample the data for hourly values, and smooth the discharge using a 72-hour moving average:
```{r prepare-data, message=FALSE, warning=FALSE}
# Resample data set for daily values
daily_upper_middle <- upper_middle %>%
  thicken("day", colname = "date") %>%
  group_by(date) %>%
  summarise(mean_daily_prcp = sum(precip_mm),
            mean_daily_prcp = ifelse(is.na(mean_daily_prcp), 
                                     0, mean_daily_prcp),
            mean_daily_q = mean(q_est, na.rm = TRUE),
            q_mm = mean_daily_q / (7.7e05) * 1000 * 86400,
            mean_daily_t = mean(air_temp, na.rm = TRUE))

# Resample data set for hourly values
hourly_upper_middle <- upper_middle %>%
  thicken("hour", colname = "dt_hour") %>%
  group_by(dt_hour) %>%
  summarise(hourly_q = mean(q_est, 
                            na.rm = TRUE) / (7.7e05) * 1000 * 3600,
            hourly_p = sum(precip_mm, na.rm = TRUE),
            hourly_t = mean(air_temp, na.rm = TRUE)) %>%
  mutate(row_id = seq(1, 3322)) %>%
  tq_mutate(select = hourly_q,   # This function populates a 72-hr moving avg
            mutate_fun = rollapply,
            width = 72,
            align = "right",
            FUN = mean,
            na.rm = TRUE,
            col_rename = "roll_mean")
```

```{r smoothed-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.asp= 0.4}
# Plot of smoothed data
hourly_upper_middle %>%
  ggplot(aes(x = dt_hour)) +
  geom_line(aes(y = hourly_q), color = "gray60", alpha = 0.5) +
  geom_line(aes(y = roll_mean), color = "darkred") +
  labs(title = "Hourly Streamflow and 72-hour Moving Average",
       subtitle = "2018 Oct - 2019 Feb",
       y = bquote("Runoff ("~mm ~hr^-1~")"), 
       x = NULL)

ggsave("figures/smoothed-q.png", width = 8.5, height = 5.5, unit = "in")
```

### Extracting recession limbs
Loop to find recession limbs that fit the criteria:

1. No significant (> 0.002 mm) rainfall in the past 24 hours
2. A decreasing Q
```{r extraction-loop}
# Counter variable — starts after 72-hr moving average begins
i <- 73

# Dummy data frame to hold row ID
df <- tibble(row_id = col_integer())

# Begin looping over hourly Q and P data
while(i < nrow(hourly_upper_middle) - 1){
  # Select rows that have < 0.002 mm 24-hour antecedent rainfall and have decreasing Q
  if(mean(hourly_upper_middle$hourly_p[c(i - 24, i)]) < 0.002 &
     hourly_upper_middle$roll_mean[i] <
     hourly_upper_middle$roll_mean[i - 1]) {
    df <- add_row(df, 
                  row_id = as.integer(i))
    i <- i + 1
  } else {
    i <- i + 1
  }
  
}
```

### Filtering recession data
Here, I narrow the data down to rainless recessions with a duration of 24 hours or more.
```{r data-filter, message=FALSE, warning=FALSE}
recession_limbs <- hourly_upper_middle %>%
  right_join(df %>% unnest(row_id))

dates1 <- recession_limbs$dt_hour
df1 <- data.frame(dates1, group = cumsum(c(TRUE, diff(dates1) != 1)))

recession_limbs <- df1 %>%
  mutate(dt_hour = as_datetime(dates1)) %>%
  left_join(recession_limbs)

consec_days <- recession_limbs %>%
  group_by(group) %>%
  tally() %>%
  filter(n >= 24) %>%
  pull(group)

recession_limbs <- recession_limbs %>%
  filter(group %in% consec_days) %>%
  select(dt_hour, recession = roll_mean, group, row_id)
```


Narrowing the data resulted in the extraction of `r {length(consec_days)}` recession periods:

```{r extracted-limbs-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 9, fig.asp= 0.4}
recession_limbs %>%
  right_join(hourly_upper_middle, by = "dt_hour") %>%
  ggplot(aes(x = dt_hour)) +
  scale_x_datetime() +
  geom_line(aes(y = hourly_q), color = "gray60", alpha = 0.5) +
  geom_line(aes(y = roll_mean), color = "darkred") +
  geom_point(aes(y = recession), color = "#0072B2", shape = 1, alpha = 0.3) +
  labs(title = "Extracted Recession Limbs",
       subtitle = "Duration > 24 hours",
       y = bquote("Streamflow ("~mm ~hr^-1~")"), 
       x = NULL)

ggsave("figures/recession-plot.png", width = 8.5, height = 5.5, units = "in")
```

### Variable timestep, after Palmroth, et al. (2010)
Application of variable timestep, using a threshold of $0.001\overline{Q}$ where $\overline{Q}$ is the mean hourly discharge over the entire dataset:

```{r variable-timestep}
threshold <- mean(hourly_upper_middle$roll_mean, na.rm = TRUE) * 0.001

# Timestep of 1 hour
one_hour_groups <- recession_limbs %>%
  group_by(group) %>%
  mutate(q_diffs = lag(recession, 1) - recession) %>%
  filter(q_diffs < threshold) %>%
  tally(group) %>%
  select(group) %>%
  pull()

one_hour_step <- recession_limbs %>%
  filter(!(group %in% one_hour_groups)) %>%
  mutate(dt = 1)

reject_groups <- one_hour_groups
# Timestep of 2 hours
two_hour_groups <- recession_limbs %>%
  group_by(group) %>%
  filter(group %in% one_hour_groups,
         row_number() %% 2 == 0) %>%
  mutate(q_diffs = lag(recession, 1) - recession) %>%
  filter(q_diffs < threshold) %>%
  tally(group) %>%
  select(group) %>%
  pull()

two_hour_step <- recession_limbs %>%
  filter(!(group %in% two_hour_groups) & group %in% one_hour_groups,
         row_number() %% 2 == 0) %>%
  mutate(dt = 2)

reject_groups <- c(reject_groups, two_hour_groups)

# Timestep of 3 hours
three_hour_groups <- recession_limbs %>%
  group_by(group) %>%
  filter(group %in% reject_groups,
         row_number() %% 3 == 1) %>%
  mutate(q_diffs = lag(recession, 1) - recession) %>%
  filter(q_diffs < threshold) %>%
  tally(group) %>%
  select(group) %>%
  pull()

three_hour_step <- recession_limbs %>%
  filter(!(group %in% three_hour_groups) & group %in% two_hour_groups,
         row_number() %% 3 == 1) %>%
  mutate(dt = 3)

reject_groups <- c(reject_groups, three_hour_groups)

# Timestep of 4 hours
four_hour_groups <- recession_limbs %>%
  group_by(group) %>%
  filter(group %in% reject_groups,
         row_number() %% 4 == 1) %>%
  mutate(q_diffs = lag(recession, 1) - recession) %>%
  filter(q_diffs < threshold) %>%
  tally(group) %>%
  select(group) %>%
  pull()

four_hour_step <- recession_limbs %>%
  filter(!(group %in% four_hour_groups) & group %in% three_hour_groups,
         row_number() %% 4 == 1) %>%
  mutate(dt = 4)
```

### Numerical differentiation
Estimating $\frac{dQ}{dt}$ and binning values for averaging:

```{r numerical-derivatives, warning=FALSE}
# Ready data for numerical derivatives
timestepped_recessions <- one_hour_step %>%
  bind_rows(two_hour_step, three_hour_step, four_hour_step) %>%
  arrange(dt_hour)

# Estimating dq_dt using central difference, after Palmroth, et al. (2010)
dq_dt <- timestepped_recessions %>%
  group_by(group) %>%
  mutate(dq = (lead(recession, 1) - lag(recession, 1)) / (2 * dt),
         q = lead(recession, 1) + lag(recession, 1) / 2,
         log_dq = log(abs(dq)),
         log_q = log(q)) %>%
  ungroup() %>%
  filter(!(is.na(dq))) %>%
  arrange(desc(q)) %>%
  select(dq, q, log_dq, log_q)


# Kirchner (2009) style binning - "Irregular Binning Method"
log_q_range <- dq_dt %>%
  summarise(log_q_range = 0.01 * (min(log_q) - max(log_q))) %>%
  pull()

bins <- c(1)

for(r in seq(1, nrow(dq_dt) - 1)) {
  
  loud = F
  min_per_bin = 35 # Chosen to improve fit of linear regression
  
  # Testing to make sure values are in set range
  if(dq_dt$log_q[r + 1] - dq_dt$log_q[r] < log_q_range){
    if(loud){print("Bin too small")}
    next
  } 
  # Testing to make sure bins are larger than min_per_bin value
  if(abs(r - bins[length(bins)]) < min_per_bin){
    if(loud){print("Not enough data points")}
    next
  }
  # Testing for bin heterogeneity
  curr = dq_dt$dq[c((bins[length(bins)]), (r + 1))]
  
  if(sd(-1 * curr) / sqrt(abs(r - bins[length(bins)])) > mean(-1 * curr)/2){
    if(loud){print("Bin too heterogenous")}
    next
  }
  bins <- c(bins, r)
}

# Binning data based on output from above loop
binned <- dq_dt %>%
  arrange(desc(log_q)) %>%
  mutate(id = row_number(),
         category = cut(id, breaks= bins, right = FALSE,
                      labels=seq(1, (length(bins)-1)))) %>%
  group_by(category) %>%
  mutate(mean_dq = mean(abs(dq)),
         se = sqrt(var(log_dq)/length(log_dq)),
         weights = 1 / sqrt(se)) %>%
  summarise(mean_dq = mean(-1 * dq),
            mean_q = mean(q),
            log_q = mean(log_q),
            log_dq = mean(log_dq),
            weights = max(weights),
            se = max(se))
```

### Weighted linear regression
I fit a quadratic curve to the binned data using a weighted linear regression, where the weights were equal to $\frac{1}{\sqrt{std. error}}$ for each binned $\log(-dQ/dt)$.

```{r wlr}
# Weighted linear regression
model <- lm(binned$log_dq ~ poly(binned$log_q, 2), 
            weights = binned$weights)

# Capture parameters and R squared from model
p0 <- number(model$coefficients[[1]], accuracy = 0.0001)
p1 <- number(model$coefficients[[2]], accuracy = 0.0001)
p2 <- number(model$coefficients[[3]], accuracy = 0.0001)
r_sq <- number(summary(model)$r.squared, accuracy = 0.001)

summary(model)
```

```{r gq-function-plot, echo=FALSE}
binned %>%
  ggplot(aes(x = log_q, y = log_dq)) +
  geom_point(data = dq_dt, aes(x = log_q, y = log_dq), 
             color = "gray60", alpha = 0.3) +
  geom_smooth(aes(x = log_q, y = log_dq, weight = weights),
            method = MASS::rlm, 
            formula = y ~ poly(x, 2), 
            se = FALSE,
            color = "darkred",
            size = 1) +
  geom_errorbar(aes(ymin = log_dq - se, ymax = log_dq + se), 
                color = "#0072B2", alpha = 0.5) +
  geom_point(color = "#0072B2") +
  scale_y_continuous() +
  scale_x_continuous() +
  labs(title = "Weighted Linear Regression of log(-dQ/dt)",
       subtitle = bquote(~.(p0)+.(p1)~"log(Q)"~+.(p2)~"log"(Q)^2~"\n"~R^2==.(r_sq)),
       y = bquote("log"~(-dQ/dt)),
       x = bquote("log"~(Q)))

ggsave("figures/wlr-plt.png", width = 8.5, height = 5.5, units = "in")
```

### Sensitivity function
The general sensitivity function $g(Q)$ follows the form:
$$g(Q) = \log\bigg(\frac{dQ/dt}{Q}\bigg) = p_0+(p_1 - 1) \log(Q) +p_2\log(Q)^2$$

The sensitivity function for the Upper Middle Ramuschaka is:
$$g(Q) = -8.4108+0.2943\log(Q)+0.2272\log(Q)^2$$
```{r define_sens_func}
p0 <- model$coefficients[[1]]
p1 <- model$coefficients[[2]] - 1
p2 <- model$coefficients[[3]]

g <- function(q, p0, p1, p2){
  g <- p0 + (p1 * log(q)) + (p2 * (log(q)^2))
  return(g)
}
```

# Estimating Evapotranspiration

I compute $E_p$ using the Hargreaves equation after Dralle et al, (2017) and Allen, Peirera, Raes and Smith (1998):

$$E_p = 0.0023\times(T_{mean}+17.8)\times(T_{max}-T_{min})^{0.5}\times0.408\times R_{est}$$

Daily $R_{est}$ was calculated using the `extrat()` function in the `sirad` package by Jedrzej S. Bojanowski, which uses the same methods described in Allen, et al. (1998).

```{r e_p-estimates}
zurite_lat <- -13.439

E_p <- hourly_upper_middle %>%
  mutate(day = yday(dt_hour),
         date = date(dt_hour)) %>%
  group_by(day) %>%
  mutate(r_est = extrat(day, zurite_lat)[[1]],
         E_p = 0.0023 * (max(hourly_t) + 17.8) * ((max(hourly_t) - min(hourly_t)) ^ 0.5) * 0.408 * r_est) %>%
  summarise(date = mean(date), r_est = mean(r_est), 
            E_p = mean(E_p)) %>%
  arrange(date)

kable(head(E_p))
```

# Determining $S_d$ and $S_i$
### Numerical integration

```{r direct_storage}
q <- daily_upper_middle$q_mm

direct_s <- function(q) {
  num <- lag(q, 1) - q
  num <- num[-1]
  denom <- g(q, p0, p1, p2)
  denom <- denom[-1]
                 
  x <- seq(1, length(num))
  y <- num / denom
  
  s_d <- c(0)
  int <- cumtrapz(x, y)
  
  s_d <- c(s_d, int)
  return(s_d)
}
```


### Cumulative values and calculation of $S_i$

```{r indirect_storage}
direct_indirect <- daily_upper_middle %>%
  right_join(E_p, by = "date") %>%
  add_column(d_s = direct_s(q)) %>%
  # If d_s < 0, I assume that it is = 0
  mutate(d_s = ifelse(d_s < 0, 0, d_s),
         cum_q = cumsum(q_mm) - q_mm[1],
         cum_p = cumsum(mean_daily_prcp),
         i_s = cum_p - cum_q - E_p - d_s,
         # Make E_p = 0 if i_s is negative
         E_p = ifelse(i_s <=0, 0, E_p),
         i_s = cum_p - cum_q - E_p - d_s,
         # Make negative i_s = zero
         i_s = ifelse(i_s < 0, 0, i_s),
         total = cum_p - cum_q - E_p,
         # Make negative total = zero
         total = ifelse(total < 0, 0 , total)) %>%
  select(date, q_mm, cum_p, cum_q, d_s, i_s, total, E_p)
```

### Plot of final results

```{r cumulative-plot, echo=F, message=F, warning=F}
direct_indirect %>%
  gather(category, measurement, cum_p:total) %>%
  ggplot(aes(x = date, y = measurement, 
             color = category, linetype = category)) +
  geom_line() +
  scale_y_log10("[mm]", 
                limits = c(1e-03, 10e3),
                breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  annotation_logticks(size = 0.5, alpha = 0.4) +
  xlab("") +
  scale_colour_discrete(name="", 
                        labels= c("\u03A3P", "\u03A3Q", 
                                  bquote("S"[d]), bquote("S"[i]),
                                  bquote("S"[T]))) +
  scale_linetype_discrete(name="", 
                        labels= c("\u03A3P", "\u03A3Q", 
                                  bquote("S"[d]), bquote("S"[i]),
                                  bquote("S"[T])))

ggsave("figures/storage-plot.png", width = 8.5, height = 5.5, units = "in")
```

```{r}
direct_indirect %>%
  mutate(cum_et = cumsum(E_p))
```

